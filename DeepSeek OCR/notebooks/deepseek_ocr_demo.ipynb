{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": "# DeepSeek OCR on Amazon SageMaker\n\nThis notebook demonstrates how to deploy and use the **DeepSeek OCR** model on Amazon SageMaker real-time endpoints.\n\n## What is DeepSeek OCR?\n\nDeepSeek OCR is a state-of-the-art vision-language model designed for optical character recognition tasks. It can:\n- Extract text from images (documents, invoices, receipts, whiteboards, etc.)\n- Convert documents to structured formats like Markdown\n- Provide bounding box coordinates for detected text (grounding mode)\n- Process both single images and multi-page PDFs\n\n**Model Details:**\n- HuggingFace: [deepseek-ai/DeepSeek-OCR](https://huggingface.co/deepseek-ai/DeepSeek-OCR)\n- Parameters: 3 billion (3B)\n- Model Size: ~6.7 GB (BF16 precision)\n- Architecture: Vision-Language Model (Image-Text-to-Text)\n- Backend: PyTorch with Transformers\n- GPU Required: Yes (we use ml.g5.2xlarge instances)\n\n## API Format\n\n### Request Body\n\nThe endpoint accepts JSON with the following fields:\n\n```json\n{\n  \"prompt\": \"<image>\\nFree OCR.\",\n  \"image_url\": \"https://example.com/image.jpg\",\n  \"max_tokens\": 8192\n}\n```\n\n**Field Options:**\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `prompt` | string | Instruction for the model. Must start with `<image>` token. |\n| `image_url` | string | URL to image (http://, https://, or s3://) |\n| `image_base64` | string | Base64-encoded image data (alternative to `image_url`) |\n| `pdf_url` | string | URL to PDF file (processes all pages) |\n| `pdf_base64` | string | Base64-encoded PDF data |\n\n**Prompt Formats:**\n- **Free OCR**: `\"<image>\\nFree OCR.\"` - Returns plain text without structure or spatial information. \"Free\" means free-form extraction with no constraints‚Äîjust raw text from the image.\n- **Grounded OCR**: `\"<image>\\n<|grounding|>Convert the document to markdown.\"` - Returns structured markdown with bounding box coordinates for each text element. Use this when you need to know where text appears in the document.\n\n### Response Body\n\n```json\n{\n  \"text\": \"Extracted text content...\",\n  \"pages\": 1\n}\n```\n\n**Response Fields:**\n- `text`: The OCR output (plain text or markdown)\n- `pages`: Number of pages processed (only present for PDFs)\n\n---\n\n## Prerequisites\n\nBefore running this notebook:\n1. Build and push the Docker image to ECR using the CodeBuild project\n2. Ensure your SageMaker execution role has permissions to:\n   - Create SageMaker models, endpoint configs, and endpoints\n   - Pull images from ECR\n   - Access S3 (if using S3 URIs for images)"
  },
  {
   "cell_type": "markdown",
   "id": "deploy-header",
   "metadata": {},
   "source": [
    "## 1. Deploy the Endpoint\n",
    "\n",
    "This section will:\n",
    "1. Create a SageMaker model pointing to our ECR image\n",
    "2. Create an endpoint configuration specifying the instance type\n",
    "3. Deploy the endpoint (takes ~5-10 minutes)\n",
    "\n",
    "The deployment process includes:\n",
    "- Pulling the Docker image from ECR\n",
    "- Starting the container on ml.g5.2xlarge instance\n",
    "- Downloading the DeepSeek OCR model from HuggingFace (~8GB)\n",
    "- Loading the model into GPU memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import time\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Setup\n",
    "region = boto3.Session().region_name\n",
    "account = boto3.client('sts').get_caller_identity()['Account']\n",
    "image = f\"{account}.dkr.ecr.{region}.amazonaws.com/deepseek-ocr-sagemaker-byoc:latest\"\n",
    "role = get_execution_role()\n",
    "sm = boto3.client('sagemaker')\n",
    "\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Account: {account}\")\n",
    "print(f\"Image URI: {image}\")\n",
    "print(f\"Role: {role}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-names",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unique names for resources\n",
    "model_name = f\"deepseek-ocr-byoc-{int(time.time())}\"\n",
    "endpoint_config_name = f\"{model_name}-cfg\"\n",
    "endpoint_name = f\"{model_name}-ep\"\n",
    "\n",
    "print(f\"Model Name: {model_name}\")\n",
    "print(f\"Endpoint Config: {endpoint_config_name}\")\n",
    "print(f\"Endpoint Name: {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SageMaker Model\n",
    "print(\"Creating SageMaker model...\")\n",
    "sm.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\n",
    "        'Image': image,\n",
    "        'Mode': 'SingleModel',\n",
    "        'Environment': {\n",
    "            'MODEL_ID': 'deepseek-ai/DeepSeek-OCR',\n",
    "            'HF_HUB_ENABLE_HF_TRANSFER': '1'\n",
    "        }\n",
    "    }\n",
    ")\n",
    "print(f\"‚úì Model created: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Endpoint Configuration\n",
    "print(\"Creating endpoint configuration...\")\n",
    "sm.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            'VariantName': 'AllTraffic',\n",
    "            'ModelName': model_name,\n",
    "            'InitialInstanceCount': 1,\n",
    "            'InstanceType': 'ml.g5.2xlarge'  # 24GB GPU, 8 vCPUs, 32GB RAM\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(f\"‚úì Endpoint config created: {endpoint_config_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-endpoint",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Endpoint (this takes ~5-10 minutes)\n",
    "print(\"Creating endpoint (this may take 5-10 minutes)...\")\n",
    "sm.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(f\"‚úì Endpoint creation started: {endpoint_name}\")\n",
    "print(\"\\nWaiting for endpoint to be in service...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wait-endpoint",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for endpoint to be ready\n",
    "waiter = sm.get_waiter('endpoint_in_service')\n",
    "waiter.wait(EndpointName=endpoint_name)\n",
    "print(f\"\\n‚úì Endpoint is ready: {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-header",
   "metadata": {},
   "source": [
    "## 2. Test the Endpoint\n",
    "\n",
    "Now that the endpoint is deployed, let's test it with different types of documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helper-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import base64\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup runtime client for inference\n",
    "runtime = boto3.client('sagemaker-runtime')\n",
    "\n",
    "def invoke_ocr(payload):\n",
    "    \"\"\"Helper function to invoke the endpoint\"\"\"\n",
    "    response = runtime.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType='application/json',\n",
    "        Body=json.dumps(payload)\n",
    "    )\n",
    "    return json.loads(response['Body'].read())\n",
    "\n",
    "print(\"‚úì Helper function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example1-header",
   "metadata": {},
   "source": [
    "### Example 1: Invoice Document\n",
    "\n",
    "This example demonstrates OCR on a standard business document (invoice). We'll use:\n",
    "- **File**: `Invoice_3.jpg` - A sample invoice with typical layout elements\n",
    "- **Prompt**: `\"<image>\\nFree OCR.\"` - Basic text extraction without formatting\n",
    "\n",
    "The model will extract all visible text from the invoice, including headers, line items, amounts, and footer information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example1-invoice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read local invoice image\n",
    "invoice_path = Path(\"Invoice_3.jpg\")\n",
    "with open(invoice_path, \"rb\") as f:\n",
    "    img_data = f.read()\n",
    "    img_base64 = base64.b64encode(img_data).decode(\"utf-8\")\n",
    "\n",
    "payload = {\n",
    "    \"prompt\": \"<image>\\nFree OCR.\",\n",
    "    \"image_base64\": img_base64\n",
    "}\n",
    "\n",
    "print(f\"Processing invoice image ({invoice_path.stat().st_size / 1024:.1f} KB)...\\n\")\n",
    "result = invoke_ocr(payload)\n",
    "\n",
    "print(\"‚úÖ SUCCESS!\\n\")\n",
    "print(\"OCR Result:\")\n",
    "print(\"=\" * 80)\n",
    "# Show first 1000 characters if output is long\n",
    "text = result[\"text\"]\n",
    "if len(text) > 1000:\n",
    "    print(text[:1000])\n",
    "    print(\"\\n... (truncated) ...\")\n",
    "else:\n",
    "    print(text)\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTotal length: {len(text)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example2-header",
   "metadata": {},
   "source": [
    "### Example 2: Handwritten Whiteboard\n",
    "\n",
    "This example shows OCR on handwritten text, which is more challenging than printed documents. We'll use:\n",
    "- **File**: `whiteboard.png` - A photo of a whiteboard with handwritten essay topics\n",
    "- **Prompt**: `\"<image>\\nFree OCR.\"` - Extract all text from the handwriting\n",
    "\n",
    "DeepSeek OCR can handle handwritten content, though accuracy depends on handwriting clarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example2-whiteboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read whiteboard image\n",
    "whiteboard_path = Path(\"whiteboard.png\")\n",
    "with open(whiteboard_path, \"rb\") as f:\n",
    "    img_data = f.read()\n",
    "    img_base64 = base64.b64encode(img_data).decode(\"utf-8\")\n",
    "\n",
    "payload = {\n",
    "    \"prompt\": \"<image>\\nFree OCR.\",\n",
    "    \"image_base64\": img_base64\n",
    "}\n",
    "\n",
    "print(f\"Processing whiteboard image ({whiteboard_path.stat().st_size / 1024:.1f} KB)...\\n\")\n",
    "result = invoke_ocr(payload)\n",
    "\n",
    "print(\"‚úÖ SUCCESS!\\n\")\n",
    "print(\"OCR Result:\")\n",
    "print(\"=\" * 80)\n",
    "text = result[\"text\"]\n",
    "if len(text) > 1000:\n",
    "    print(text[:1000])\n",
    "    print(\"\\n... (truncated) ...\")\n",
    "else:\n",
    "    print(text)\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTotal length: {len(text)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example3-header",
   "metadata": {},
   "source": [
    "### Example 3: Markdown Conversion with Grounding\n",
    "\n",
    "This example demonstrates advanced features:\n",
    "- **Grounding mode**: Returns bounding box coordinates for each text element\n",
    "- **Markdown output**: Structures the content with formatting\n",
    "\n",
    "The output includes `<|ref|>` and `<|det|>` tags with coordinates in format `[[x1, y1, x2, y2]]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example3-grounding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the whiteboard image with grounding prompt\n",
    "whiteboard_path = Path(\"whiteboard.png\")\n",
    "with open(whiteboard_path, \"rb\") as f:\n",
    "    img_data = f.read()\n",
    "    img_base64 = base64.b64encode(img_data).decode(\"utf-8\")\n",
    "\n",
    "payload = {\n",
    "    \"prompt\": \"<image>\\n<|grounding|>Convert the document to markdown.\",\n",
    "    \"image_base64\": img_base64\n",
    "}\n",
    "\n",
    "print(\"Processing with grounding mode (bounding boxes)...\\n\")\n",
    "result = invoke_ocr(payload)\n",
    "\n",
    "print(\"‚úÖ SUCCESS!\\n\")\n",
    "print(\"OCR Result with Bounding Boxes:\")\n",
    "print(\"=\" * 80)\n",
    "text = result[\"text\"]\n",
    "# Show first 800 characters to see the format\n",
    "print(text[:800])\n",
    "if len(text) > 800:\n",
    "    print(\"\\n... (truncated) ...\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTotal length: {len(text)} characters\")\n",
    "print(\"\\nNote: <|det|> tags contain bounding box coordinates [x1, y1, x2, y2]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example4-header",
   "metadata": {},
   "source": [
    "### Example 4: PDF Processing (News Article)\n",
    "\n",
    "This example demonstrates PDF processing with a real-world document. PDFs are handled through a multi-step process:\n",
    "\n",
    "**How PDF Processing Works:**\n",
    "1. **Server receives PDF** - As base64 encoded data or URL\n",
    "2. **pypdfium2 library** - Renders each PDF page as a 200 DPI image\n",
    "3. **Sequential processing** - Each rendered image goes through DeepSeek-OCR\n",
    "4. **Combined results** - Server returns concatenated text with page markers\n",
    "\n",
    "**Important**: The DeepSeek-OCR model itself only processes images. PDF handling is done by our FastAPI server, NOT by the model.\n",
    "\n",
    "**File**: `1706.03762v7.pdf`\n",
    "\n",
    "**Note**: Multi-page PDFs may take longer to process. Real-time endpoints have a 60-second timeout, so very large PDFs may timeout. For production use with large PDFs, consider async endpoints or batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example4-pdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Read PDF file\n",
    "pdf_path = Path(\"1706.03762v7.pdf\")\n",
    "\n",
    "print(f\"Processing PDF: {pdf_path.name}\")\n",
    "print(f\"File size: {pdf_path.stat().st_size / 1024 / 1024:.1f} MB\\n\")\n",
    "\n",
    "with open(pdf_path, \"rb\") as f:\n",
    "    pdf_data = f.read()\n",
    "    pdf_base64 = base64.b64encode(pdf_data).decode(\"utf-8\")\n",
    "\n",
    "payload = {\n",
    "    \"prompt\": \"<image>\\nFree OCR.\",\n",
    "    \"pdf_base64\": pdf_base64\n",
    "}\n",
    "\n",
    "print(\"‚ö†Ô∏è  Note: This may take 10-30 seconds depending on page count...\\n\")\n",
    "print(\"Starting OCR processing...\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    result = invoke_ocr(payload)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n‚úÖ SUCCESS!\")\n",
    "    print(f\"Processed {result.get('pages', 'unknown')} pages in {elapsed:.1f} seconds\")\n",
    "    print(f\"Average: {elapsed/result.get('pages', 1):.1f} seconds per page\\n\")\n",
    "    \n",
    "    print(\"OCR Result (first 1000 characters):\")\n",
    "    print(\"=\" * 80)\n",
    "    text = result['text']\n",
    "    print(text[:1000])\n",
    "    if len(text) > 1000:\n",
    "        print(\"\\n... (truncated) ...\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nTotal output length: {len(text)} characters\")\n",
    "    print(f\"Pages processed: {result.get('pages', 'N/A')}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\n‚ùå Error after {elapsed:.1f} seconds: {str(e)}\")\n",
    "    print(\"\\nüí° Tip: If the PDF has many pages, consider processing pages individually to avoid timeout.\")\n",
    "    print(\"See the 'Processing Large PDFs' section below for an example.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup-header",
   "metadata": {},
   "source": [
    "## 3. Cleanup Resources\n",
    "\n",
    "**Important**: SageMaker endpoints incur charges while running. Delete the endpoint when you're done to stop charges.\n",
    "\n",
    "**Costs**:\n",
    "- ml.g5.2xlarge: ~$1.52/hour\n",
    "- Model artifacts in ECR: minimal storage costs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-resources",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display current resources\n",
    "print(\"Current resources:\")\n",
    "print(f\"  Endpoint: {endpoint_name}\")\n",
    "print(f\"  Endpoint Config: {endpoint_config_name}\")\n",
    "print(f\"  Model: {model_name}\")\n",
    "print(\"\\nRun the next cell to delete these resources.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete endpoint, config, and model\n",
    "try:\n",
    "    print(\"Deleting endpoint...\")\n",
    "    sm.delete_endpoint(EndpointName=endpoint_name)\n",
    "    print(f\"‚úì Endpoint deleted: {endpoint_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Could not delete endpoint: {e}\")\n",
    "\n",
    "try:\n",
    "    print(\"\\nDeleting endpoint configuration...\")\n",
    "    sm.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "    print(f\"‚úì Endpoint config deleted: {endpoint_config_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Could not delete endpoint config: {e}\")\n",
    "\n",
    "try:\n",
    "    print(\"\\nDeleting model...\")\n",
    "    sm.delete_model(ModelName=model_name)\n",
    "    print(f\"‚úì Model deleted: {model_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Could not delete model: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úì Cleanup completed!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "- ‚úì Deploying DeepSeek OCR on SageMaker with PyTorch/Transformers backend\n",
    "- ‚úì Processing business documents (invoices)\n",
    "- ‚úì Handling handwritten text (whiteboards)\n",
    "- ‚úì Using grounding mode for bounding box detection\n",
    "- ‚úì Processing multi-page PDFs\n",
    "- ‚úì Cleaning up resources\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Model Performance**: DeepSeek OCR handles both printed and handwritten text\n",
    "2. **Flexible Input**: Accepts images (URL, S3, base64) and PDFs\n",
    "3. **Output Formats**: Plain text or structured markdown with bounding boxes\n",
    "4. **Instance Type**: ml.g5.2xlarge provides good balance of performance and cost\n",
    "5. **Timeout Considerations**: Real-time endpoints have 60s limit, plan accordingly for large documents\n",
    "\n",
    "### Use Cases:\n",
    "\n",
    "- **Document Digitization**: Converting scanned documents to searchable text\n",
    "- **Invoice Processing**: Extracting data from business documents\n",
    "- **Receipt OCR**: Expense tracking and automation\n",
    "- **Form Processing**: Extracting information from structured forms\n",
    "- **Whiteboard Capture**: Digitizing meeting notes and brainstorming sessions\n",
    "\n",
    "### Resources:\n",
    "\n",
    "- **Model**: [DeepSeek-AI/DeepSeek-OCR on HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-OCR)\n",
    "- **SageMaker BYOC Guide**: [AWS Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms.html)\n",
    "- **Instance Pricing**: [SageMaker Pricing](https://aws.amazon.com/sagemaker/pricing/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}